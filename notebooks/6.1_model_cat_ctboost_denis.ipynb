{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "##### Projet CO2 par Polina, Vincent, Denis\n",
    "\n",
    "Ce notebook:\n",
    "entraine un mod√®le de classification pour pr√©diction par Gradient Boosting Machines Catboost  \n",
    "Prend en entr√©e les fichiers:\n",
    "    (processed)/X_test_scaled.csv, X_train_scaled.csv, y_test_cat.csv, y_train_cat.csv : les donn√©es scal√©es et donc forc√©ment pr√©alablement s√©par√©es en jeux de train/test.\n",
    "\n",
    "Fournit en sortie les fichiers:\n",
    "    (models)/<nom_de_modele>.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charge les chemins vers les fichiers de donn√©es : base_processed, base_raw, base_models...\n",
    "%run init_notebook.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_co2 import load_our_data_cat, display_norm_matrix, display_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification par Gradient Boosting Machines Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"16\"  # nombre de c≈ìurs physiques r√©els\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test=load_our_data_cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette case est une r√©f√©rence pour les hyperparam√®tres du mod√®le\n",
    "hyperparams = {\n",
    "    'iterations': 1000,  # Nombre d'it√©rations\n",
    "    'learning_rate': 0.1,  # Taux d'apprentissage\n",
    "    'depth': 6,  # Profondeur des arbres\n",
    "    'l2_leaf_reg': 3,  # R√©gularisation L2\n",
    "    'random_seed': 42,  # Seed pour la reproductibilit√©\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialiser le mod√®le\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhyperparams)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "model = CatBoostClassifier(**hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entra√Æner le mod√®le\n",
    "def train_model(model, X_train_scaled, y_train):\n",
    "    \"\"\"\n",
    "    Entra√Æne le mod√®le.\n",
    "    \"\"\"\n",
    "    model.fit(X_train_scaled, y_train, verbose=0)  # verbose=0 pour √©viter trop de logs\n",
    "    return model\n",
    "\n",
    "# Fonction pour √©valuer les performances du mod√®le\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score\n",
    "def evaluate_model(model, X_test_scaled, y_test):\n",
    "    \"\"\"\n",
    "    Calcule et affiche les m√©triques du mod√®le.\n",
    "    \"\"\"\n",
    "    # Pr√©dictions sur les donn√©es de test\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcul et affichage de la pr√©cision\n",
    "    accuracy = model.score(X_test_scaled, y_test)\n",
    "    print(f\"Pr√©cision du mod√®le : {accuracy:.2f}\")\n",
    "    \n",
    "    # Calcul et affichage du F1-score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' pour les classes d√©s√©quilibr√©es\n",
    "    print(f\"F1-score : {f1:.2f}\")\n",
    "    \n",
    "    # Calcul et affichage du recall\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"Recall : {recall:.2f}\")\n",
    "    \n",
    "    # Affichage du rapport de classification\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"Rapport de classification :\")\n",
    "    print(report)\n",
    "\n",
    "    return accuracy, f1, recall, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info pour les repr√©sentations graphiques\n",
    "name=\"CatBoost\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour afficher la matrice de confusion\n",
    "def display_confusion_matrix(model, X_test_scaled, y_test, name=\"Mod√®le\", params=None):\n",
    "    \"\"\"\n",
    "    Affiche la matrice de confusion normalis√©e.\n",
    "    \"\"\"\n",
    "    # Pr√©dictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Matrice de confusion brute\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Normalisation par le nombre total de chaque classe\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    print(f\"\\nüîπ Matrice de confusion pour {name} avec {params} üîπ\")\n",
    "\n",
    "    # Affichage\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Heatmap avec les deux annotations\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "\n",
    "    \"\"\"\n",
    "    # si version trop r√©cente de matplotlib, ajout des valeurs sur une deuxi√®me couche d'annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j + 0.5, i + 0.5, f\"{cm[i, j]}\",\n",
    "                     ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "    \"\"\"\n",
    "\n",
    "    plt.xlabel(\"Pr√©dictions\")\n",
    "    plt.ylabel(\"Vraies classes\")\n",
    "    plt.title(f\"Matrice de confusion normalis√©e, {name}\")\n",
    "    plt.show()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=train_model(model, X_train_scaled, y_train)\n",
    "accuracy, f1, recall, y_pred=evaluate_model(model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyperparams = f\"iterations={hyperparams['iterations']}, depth={hyperparams['depth']}, l2_leaf_reg={hyperparams['l2_leaf_reg']}\"\n",
    "display_norm_matrix(name, y_pred, y_test, text_hyperparams)\n",
    "# display_confusion_matrix(model, X_test_scaled, y_test, name=name, params=hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la qualit√© du r√©sultat ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouveau set d'hyperparam√®tres du mod√®le\n",
    "hyperparams = {\n",
    "    'iterations': 1500,  # Nombre d'it√©rations\n",
    "    'learning_rate': 0.1,  # Taux d'apprentissage\n",
    "    'depth': 6,  # Profondeur des arbres\n",
    "    'l2_leaf_reg': 2,  # R√©gularisation L2\n",
    "    'random_seed': 42,  # Seed pour la reproductibilit√©\n",
    "}\n",
    "\n",
    "model = CatBoostClassifier(**hyperparams)\n",
    "model=train_model(model, X_train_scaled, y_train)\n",
    "accuracy, f1, recall, y_pred=evaluate_model(model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyperparams = f\"iterations={hyperparams['iterations']}, depth={hyperparams['depth']}, l2_leaf_reg={hyperparams['l2_leaf_reg']}\"\n",
    "display_norm_matrix(name, y_pred, y_test, text_hyperparams)\n",
    "# display_confusion_matrix(model, X_test_scaled, y_test, name=name, params=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouveau set d'hyperparam√®tres du mod√®le\n",
    "hyperparams = {\n",
    "    'iterations': 1500,  # Nombre d'it√©rations\n",
    "    'learning_rate': 0.1,  # Taux d'apprentissage\n",
    "    'depth': 6,  # Profondeur des arbres\n",
    "    'l2_leaf_reg': 2,  # R√©gularisation L2\n",
    "    'random_seed': 42,  # Seed pour la reproductibilit√©\n",
    "}\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "model = CatBoostClassifier(**hyperparams)\n",
    "model=train_model(model, X_train_resampled, y_train_resampled)\n",
    "accuracy, f1, recall, y_pred=evaluate_model(model, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyperparams = f\"iterations={hyperparams['iterations']}, depth={hyperparams['depth']}, l2_leaf_reg={hyperparams['l2_leaf_reg']}\"\n",
    "text_hyperparams += f\", SMOTE\"\n",
    "display_norm_matrix(name, y_pred, y_test, text_hyperparams)\n",
    "# display_confusion_matrix(model, X_test_scaled, y_test, name=name, params=hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_roc(X_test_scaled, y_test, y_pred, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentative d'am√©lioration par p√©nalit√©\n",
    "\n",
    "La classe 2 est sous repr√©sent√©e par rapport √† la classe 3,  \n",
    "on applique donc une p√©nalit√© aux probabilit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param√®tres pour ce chapitre:\n",
    "threshold = 0.2 # par exemple 0.1 pour 10% de favorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dire les classes sur les donn√©es de test\n",
    "y_prob = model.predict_proba(X_test_scaled)\n",
    "\n",
    "y_adjusted_pred = []\n",
    "\n",
    "# Custom logic: on priorisera class 2 si c'est celle qui est la plus probable au threshold pr√®s\n",
    "for prob in y_prob:\n",
    "    # La classe avec la plus haute probe est:\n",
    "    max_prob_class_index = np.argmax(prob)\n",
    "\n",
    "    # Check if class 2 is close enough to the maximum probability\n",
    "    if prob[1] >= prob[max_prob_class_index] - threshold:  # Close enough to the max probability\n",
    "        y_adjusted_pred.append(2)  # Favor class 2\n",
    "    else:\n",
    "        y_adjusted_pred.append(max_prob_class_index+1)  # Stick to the class with the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for info \n",
    "display(y_prob)\n",
    "# k-NN with k=5 (our best results) is not very good for improvement by penalty,\n",
    "# because the granularity of the probabilities is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "# Calculate new metrics\n",
    "adjusted_accuracy = accuracy_score(y_test, y_adjusted_pred)\n",
    "adjusted_f1 = f1_score(y_test, y_adjusted_pred, average='weighted')\n",
    "adjusted_recall = recall_score(y_test, y_adjusted_pred, average='weighted')  # Include recall calculation\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Adjusted Accuracy: {adjusted_accuracy:.2f}\")\n",
    "print(f\"Adjusted F1-Score: {adjusted_f1:.2f}\")\n",
    "print(f\"Adjusted Recall: {adjusted_recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour info, les lignes qui ont boug√© avec la p√©nalisation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to compare the Series\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Original Prediction\": y_pred,\n",
    "    \"Adjusted Prediction\": y_adjusted_pred\n",
    "})\n",
    "\n",
    "# Add a column to indicate differences\n",
    "comparison_df[\"Difference\"] = comparison_df[\"Original Prediction\"] != comparison_df[\"Adjusted Prediction\"]\n",
    "\n",
    "# Display rows with differences\n",
    "differences = comparison_df[comparison_df[\"Difference\"]]\n",
    "display(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterations=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, l2_leaf_reg=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_leaf_reg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m text_hyperparams \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, SMOTE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m display_norm_matrix(name, \u001b[43my_pred\u001b[49m, y_test, text_hyperparams)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# display_confusion_matrix(model, X_test_scaled, y_test, name=name, params=hyperparams)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# affichage de la matrice\n",
    "text_hyperparams = f\"iterations={hyperparams['iterations']}, depth={hyperparams['depth']}, l2_leaf_reg={hyperparams['l2_leaf_reg']}\"\n",
    "text_hyperparams += f\", p√©nalit√© {threshold} %\"\n",
    "display_norm_matrix(name, y_pred, y_test, text_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Chemin pour enregistrer le mod√®le\n",
    "model_path = base_models + 'rf_cat.pkl'\n",
    "\n",
    "# Enregistrer le mod√®le\n",
    "joblib.dump(random_forest_model, model_path)\n",
    "\n",
    "print(f\"Mod√®le random_forest enregistr√© dans {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
